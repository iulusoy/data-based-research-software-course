{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo notebook for model learning using scikit-learn\n",
    "We use the journal ranking dataset to learn correlations in the data using linear regression. See also [here](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py) and [here](https://scikit-learn.org/stable/common_pitfalls.html) for references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import at the top of the notebook\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    RepeatedKFold,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.compose import TransformedTargetRegressor, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    PredictionErrorDisplay,\n",
    "    mean_absolute_error,\n",
    "    median_absolute_error,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing steps as per the data notebook\n",
    "raw_dataset = pd.read_csv(\"../data/journal_ranking_data.csv\")\n",
    "clean_dataset = raw_dataset.drop(raw_dataset[raw_dataset[\"CiteScore\"].gt(100)].index)\n",
    "clean_dataset = clean_dataset.drop(\n",
    "    clean_dataset[clean_dataset[\"Cites/Doc. 2y\"].gt(43)].index\n",
    ")\n",
    "clean_dataset = clean_dataset.dropna()\n",
    "clean_dataset = clean_dataset.drop_duplicates(subset=[\"CiteScore\", \"Cites/Doc. 2y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(clean_dataset, x=\"CiteScore\", nbins=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(clean_dataset, x=\"Cites/Doc. 2y\", nbins=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test set\n",
    "train, test = train_test_split(clean_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the two columns to np arrays\n",
    "X = train[\"Cites/Doc. 2y\"].to_numpy().reshape(-1, 1)\n",
    "y = train[\"CiteScore\"].to_numpy().reshape(-1, 1)\n",
    "X_test = test[\"Cites/Doc. 2y\"].to_numpy().reshape(-1, 1)\n",
    "y_test = test[\"CiteScore\"].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear regression model\n",
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coefficients\n",
    "reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the linear relationship using the predicted coefficients\n",
    "xvals = np.arange(0, 60, 1)\n",
    "yvals = reg.predict(xvals.reshape(-1, 1))\n",
    "yvals[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "fig = px.scatter(train, x=\"Cites/Doc. 2y\", y=\"CiteScore\")\n",
    "fig.add_trace(go.Scatter(x=xvals, y=yvals[:, 0]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train[[\"Cites/Doc. 2y\", \"CiteScore\"]], kind=\"reg\", diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = mean_absolute_error(y, reg.predict(X))\n",
    "y_test_predict = reg.predict(X_test)\n",
    "mae_test = mean_absolute_error(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"MAE on training set\": f\"{mae_train:.1f}\",\n",
    "    \"MAE on testing set\": f\"{mae_test:.1f}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "display = PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_test_predict,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    ax=ax,\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "ax.set_title(\"Linear\")\n",
    "for name, score in scores.items():\n",
    "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now include more features in the fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features for the training\n",
    "selected_columns = [\n",
    "    \"SJR-index\",\n",
    "    \"H-index\",\n",
    "    \"Total Docs.\",\n",
    "    \"Total Docs. 3y\",\n",
    "    \"Total Refs.\",\n",
    "    \"Total Cites 3y\",\n",
    "    \"Citable Docs. 3y\",\n",
    "    \"Cites/Doc. 2y\",\n",
    "    \"Refs./Doc.\",\n",
    "]\n",
    "multiple_features = clean_dataset[selected_columns + [\"CiteScore\"]].copy()\n",
    "# drop duplicates\n",
    "multiple_features = multiple_features.drop_duplicates()\n",
    "train, test = train_test_split(multiple_features, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n",
    "# we only have to deal with numerical data and do not need to map any categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which features should be trained for and which should be predicted\n",
    "X = train[selected_columns]\n",
    "y = train[\"CiteScore\"]\n",
    "X_test = test[selected_columns]\n",
    "y_test = test[\"CiteScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train, kind=\"reg\", diag_kind=\"kde\")\n",
    "plt.show()\n",
    "# distribution for CiteScore has a long tail, could take the log to\n",
    "# approximate a normal distribution\n",
    "# also shows clearly which values are correlated by linear relationship\n",
    "# as we saw previously, Cites/Doc. 2y is linearly correlated with CiteScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the numerical columns for a more balanced representation in the model weights\n",
    "scale_columns = selected_columns\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), scale_columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the model architecture: a linear model using ridge regression and a very small\n",
    "# regularization parameter\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=linear_model.Ridge(alpha=1e-10),\n",
    "    ),\n",
    ")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the fit method on the model\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the model performance\n",
    "mae_train = median_absolute_error(y, model.predict(X))\n",
    "y_pred = model.predict(X_test)\n",
    "mae_test = median_absolute_error(y_test, y_pred)\n",
    "scores = {\n",
    "    \"MedAE on training set\": f\"{mae_train:.2f} CiteScore\",\n",
    "    \"MedAE on testing set\": f\"{mae_test:.2f} CiteScore\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "display = PredictionErrorDisplay.from_predictions(\n",
    "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
    ")\n",
    "ax.set_title(\"Ridge model, small regularization\")\n",
    "for name, score in scores.items():\n",
    "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Moderate performance as the predicted values lie around the mark of the actual values. \n",
    "- The model is not overfitting as the performance on the test set is similar to the training set. \n",
    "- The model is not underfitting as the performance is not significantly worse than the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model[:-1].get_feature_names_out()\n",
    "\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    model[-1].regressor_.coef_,\n",
    "    columns=[\"Coefficients importance\"],\n",
    "    index=feature_names,\n",
    ")\n",
    "coefs.plot.barh(figsize=(9, 7))\n",
    "plt.title(\"Ridge model, small regularization, normalized variables\")\n",
    "plt.xlabel(\"Raw coefficient values\")\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The values of the coefficients seem to indicate that Cites/Docs. 2y, Citable Docs. 3y and Total Docs. 3y have most significance on the prediced value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain using cross-validation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\n",
    "cv_model = cross_validate(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    return_estimator=True,\n",
    "    n_jobs=2,\n",
    "    scoring=[\"neg_mean_absolute_error\"],\n",
    ")\n",
    "coefs = pd.DataFrame(\n",
    "    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "sns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
    "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.title(\"Coefficient variability\")\n",
    "plt.subplots_adjust(left=0.3)\n",
    "plt.savefig(\"coeff_variability.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coefficient importance and variability over several folds: The features Citable Docs. 3y and Total Docs. 3y show vastly changing coefficients. This is due to their interdependence: Their effect on the value to be predicted is difficult to treat separately (collinear features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train_average = abs(np.mean(cv_model[\"test_neg_mean_absolute_error\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross validation to predict the test set\n",
    "# this is usually not done, but for demonstration purposes\n",
    "y_pred = cross_val_predict(model, X_test, y_test, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = mean_absolute_error(y_test, y_pred)\n",
    "scores = {\n",
    "    \"MAE on training set\": f\"{mae_train_average:.2f} CiteScore\",\n",
    "    \"MAE on testing set\": f\"{mae_test:.2f} CiteScore\",\n",
    "}\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "display = PredictionErrorDisplay.from_predictions(\n",
    "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
    ")\n",
    "ax.set_title(\"Ridge model, small regularization, across five folds\")\n",
    "for name, score in scores.items():\n",
    "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
