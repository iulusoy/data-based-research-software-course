{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo notebook for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import TransformedTargetRegressor, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import PredictionErrorDisplay, mean_squared_error, r2_score, mean_absolute_error, root_mean_squared_error, median_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_csv(\"../data/journal_ranking_data.csv\")\n",
    "clean_dataset = raw_dataset.drop(raw_dataset[raw_dataset[\"CiteScore\"].gt(100)].index)\n",
    "clean_dataset = clean_dataset.dropna()\n",
    "clean_dataset = clean_dataset.drop_duplicates(subset=[\"CiteScore\", \"Cites/Doc. 2y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(clean_dataset, x=\"CiteScore\", nbins=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(clean_dataset, x=\"Cites/Doc. 2y\", nbins=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(clean_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the two columns to np arrays\n",
    "X = train[\"Cites/Doc. 2y\"].to_numpy().reshape(-1, 1)\n",
    "y = train[\"CiteScore\"].to_numpy().reshape(-1, 1)\n",
    "X_test = test[\"Cites/Doc. 2y\"].to_numpy().reshape(-1, 1)\n",
    "y_test = test[\"CiteScore\"].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.arange(0, 100, 1)\n",
    "yvals = reg.predict(xvals.reshape(-1, 1))\n",
    "yvals[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "fig = px.scatter(train, x=\"Cites/Doc. 2y\", y=\"CiteScore\")\n",
    "fig.add_trace(go.Scatter(x=xvals, y=yvals[:,0]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[[\"Cites/Doc. 2y\", \"CiteScore\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train[[\"Cites/Doc. 2y\", \"CiteScore\"]], kind=\"reg\", diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = median_absolute_error(y, reg.predict(X))\n",
    "y_test_predict = reg.predict(X_test)\n",
    "mae_test = median_absolute_error(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    \"MedAE on training set\": f\"{mae_train:.1f}\",\n",
    "    \"MedAE on testing set\": f\"{mae_test:.1f}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "display = PredictionErrorDisplay.from_predictions(\n",
    "    y_test, y_test_predict, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
    ")\n",
    "ax.set_title(\"Linear\")\n",
    "for name, score in scores.items():\n",
    "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now include more info in the fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"SJR-index\", \"H-index\", \"Total Docs.\", \"Total Docs. 3y\", \"Total Refs.\", \"Total Cites 3y\", \"Citable Docs. 3y\", \"Cites/Doc. 2y\", \"Refs./Doc.\"]\n",
    "multiple_features = clean_dataset[selected_columns + [\"CiteScore\"]].copy()\n",
    "# drop duplicates\n",
    "multiple_features = multiple_features.drop_duplicates()\n",
    "train, test = train_test_split(multiple_features, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[selected_columns]\n",
    "y = train[\"CiteScore\"]\n",
    "X_test = test[selected_columns]\n",
    "y_test = test[\"CiteScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train, kind=\"reg\", diag_kind=\"kde\")\n",
    "plt.show()\n",
    "# distribution for CiteScore has a long tail, could take the log to \n",
    "# approximate a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns = selected_columns\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), scale_columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=linear_model.Ridge(alpha=1e-10),\n",
    "    ),\n",
    ")\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_train = median_absolute_error(y, model.predict(X))\n",
    "y_pred = model.predict(X_test)\n",
    "mae_test = median_absolute_error(y_test, y_pred)\n",
    "scores = {\n",
    "    \"MedAE on training set\": f\"{mae_train:.2f} CiteScore\",\n",
    "    \"MedAE on testing set\": f\"{mae_test:.2f} CiteScore\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "display = PredictionErrorDisplay.from_predictions(\n",
    "    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
    ")\n",
    "ax.set_title(\"Ridge model, small regularization\")\n",
    "for name, score in scores.items():\n",
    "    ax.plot([], [], \" \", label=f\"{name}: {score}\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model[:-1].get_feature_names_out()\n",
    "\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    model[-1].regressor_.coef_,\n",
    "    columns=[\"Coefficients importance\"],\n",
    "    index=feature_names,\n",
    ")\n",
    "coefs.plot.barh(figsize=(9, 7))\n",
    "plt.title(\"Ridge model, small regularization, normalized variables\")\n",
    "plt.xlabel(\"Raw coefficient values\")\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cross_validate(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    return_estimator=True,\n",
    "    n_jobs=2,\n",
    ")\n",
    "coefs = pd.DataFrame(\n",
    "    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "sns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\n",
    "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.title(\"Coefficient variability\")\n",
    "plt.subplots_adjust(left=0.3)\n",
    "plt.savefig(\"coeff_variability.png\")\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful when implying feature importance due to the coefficients!\n",
    "Coefficients need to be normalized to compare feature importance.\n",
    "Machine Learning models are generally unable to infer causal effects because of the likelihood of  unobserved confounding variables that either inflate or deflate that coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
